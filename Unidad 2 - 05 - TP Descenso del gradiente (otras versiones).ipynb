{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["X = 2 * np.random.rand(100,1)\n","y = 4 +3 * X+np.random.randn(100,1)"],"metadata":{"trusted":true,"id":"DNMs_w7045hA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Descenso del gradiente\n","\n","Abordamos el mismo problema de ajustar los datos según una recta, pero en este caso, en lugar de acudir a la fórmula cerrada que nos dan las ecuaciones normales para recuperar el valor de $\\theta$ óptimo, aplicaremos descenso del gradiente a nuestro problema de minimización.\n","\n","Ejercicio: Escribir el problema como problema de optimización, explicitando función objetivo y calcular su gradiente."],"metadata":{"id":"yPY3Coev45hB"}},{"cell_type":"markdown","source":["A continuación armamos la función objetivo que utilizaremos para evaluar cada iteración del algoritmo."],"metadata":{"id":"lFyoPbEm9VQ1"}},{"cell_type":"code","source":["\n","def  cal_cost(theta,X,y):\n","    '''\n","\n","    Calcula el costo para X e y dados. Lo siguiente muestra un ejemplo de un X unidimensional.\n","    theta = Vector de thetas\n","    X     = Fila de X's np.zeros((2,j))\n","    y     = y's reales np.zeros((2,1))\n","\n","    dond:\n","        j es la cantidad de características (features)\n","    '''\n","\n","    m = len(y)\n","\n","    predictions = X.dot(theta)\n","    cost = (1/2*m) * np.sum(np.square(predictions-y))\n","    return cost\n"],"metadata":{"trusted":true,"id":"mlj9J0hl45hB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora estamos en condiciones de armar el algoritmo del descenso del gradiente de para aplicar a nuestro problema de mínimo."],"metadata":{"id":"_IffHfMtDBRL"}},{"cell_type":"code","source":["def gradient_descent(X,y,theta,learning_rate=0.01,iterations=100):\n","    '''\n","    X    = Matriz de X con columna de unos\n","    y    = Vector de Y\n","    theta= Vector de thetas\n","    learning_rate= longitud del paso (tasa de aprendizaje)\n","    iterations = cantidad de iteraciones\n","\n","    Devuelve el vector theta final y el vector de evolución del costo a lo largo de las iteraciones\n","    '''\n","    m = len(y)\n","    cost_history = np.zeros(iterations)\n","    theta_history = np.zeros((iterations,2))\n","    for it in range(iterations):\n","\n","        prediction = np.dot(X,theta)\n","\n","        theta = theta -(1/m)*learning_rate*( X.T.dot((prediction - y)))\n","        theta_history[it,:] =theta.T\n","        cost_history[it]  = cal_cost(theta,X,y)\n","\n","    return theta, cost_history, theta_history"],"metadata":{"trusted":true,"id":"tfpX3BCw45hC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Probemos nuestro algoritmo con algún valor fijo para la tasa de aprendizaje, alguna cantidad de iteraciones y algún valor inicial del vector $\\theta$."],"metadata":{"id":"qp1f5mbbE5ZA"}},{"cell_type":"code","source":["lr =0.01\n","n_iter = 1000\n","\n","theta = np.random.randn(2,1)\n","\n","X_b = np.c_[np.ones((len(X),1)),X]\n","theta,cost_history,theta_history = gradient_descent(X_b,y,theta,lr,n_iter)\n","\n","\n"],"metadata":{"trusted":true,"id":"bkbT4Gpt45hC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["T=[theta_history[100*k] for k in range(10)]\n","print(T)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zbKUBM2SID8o","executionInfo":{"status":"ok","timestamp":1697571197806,"user_tz":180,"elapsed":2,"user":{"displayName":"Jorgelina Walpen","userId":"08891538883750565205"}},"outputId":"0569adfc-4ee8-42fd-8baf-61de7ce313b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[array([ 0.77160721, -0.72021254]), array([3.59776137, 2.57893735]), array([3.92643947, 2.95924133]), array([3.96587988, 3.00203894]), array([3.97163228, 3.00597394]), array([3.97330025, 3.00557178]), array([3.97433838, 3.0048023 ]), array([3.97517107, 3.00410372]), array([3.9758682 , 3.00350896]), array([3.97645541, 3.00300684])]\n"]}]},{"cell_type":"code","source":["L=[cost_history[100*k] for k in range(10)]\n","print(L)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lkqybG9KHP6s","executionInfo":{"status":"ok","timestamp":1697570982859,"user_tz":180,"elapsed":321,"user":{"displayName":"Jorgelina Walpen","userId":"08891538883750565205"}},"outputId":"10c6e0e7-c2f9-45fa-caf3-aba9f429a4d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[150942.58459616284, 7402.403903380769, 5293.1211813636, 5131.555695847219, 5034.614498010824, 4965.941311135096, 4917.1265307306085, 4882.425409934301, 4857.757282002872, 4840.2213484190215]\n"]}]},{"cell_type":"markdown","source":["# Descenso del gradiente Estocástico"],"metadata":{"id":"Tzg7Ltn-45hD"}},{"cell_type":"code","source":["def stocashtic_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10):\n","    '''\n","    X    = Matriz de X\n","    y    = Vector de Y\n","    theta= Vector de thetas\n","    learning_rate: tasa de aprendizaje\n","    iterations = cant de iteraciones\n","\n","    Devuelve el vector final de thetas y un array con la evolución del costo a lo largo de las iteraciones\n","    '''\n","    m = len(y)\n","    cost_history = np.zeros(iterations)\n","\n","\n","    for it in range(iterations):\n","        cost =0.0\n","        for i in range(m):\n","            rand_ind = np.random.randint(0,m)\n","            X_i = X[rand_ind,:].reshape(1,X.shape[1])\n","            y_i = y[rand_ind].reshape(1,1)\n","            prediction = np.dot(X_i,theta)\n","\n","            theta = theta -(1/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n","            cost += cal_cost(theta,X_i,y_i)\n","        cost_history[it]  = cost\n","\n","    return theta, cost_history"],"metadata":{"trusted":true,"id":"g8QY2Of445hD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["¿Cómo se está haciendo la \"evaluación\" del gradiente? ¿Cómo se está haciendo la actualización de \"los\" thetas?"],"metadata":{"id":"ND4xDX_pZFxW"}},{"cell_type":"code","source":["lr =0.5\n","n_iter = 50\n","\n","theta = np.random.randn(2,1)\n","\n","X_b = np.c_[np.ones((len(X),1)),X]\n","theta,cost_history = stocashtic_gradient_descent(X_b,y,theta,lr,n_iter)\n","\n","\n","print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n","print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))"],"metadata":{"trusted":true,"id":"zNmsE35x45hD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,ax = plt.subplots(figsize=(10,8))\n","\n","ax.set_ylabel('{J(Theta)}',rotation=0)\n","ax.set_xlabel('{Iterations}')\n","theta = np.random.randn(2,1)\n","\n","_=ax.plot(range(n_iter),cost_history,'b.')"],"metadata":{"trusted":true,"id":"ld0VScka45hD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Descenso del gradiente tipo Mini Batch"],"metadata":{"id":"9dk8YArD45hE"}},{"cell_type":"code","source":["def minibatch_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10,batch_size =20):\n","    '''\n","    X    = Matriz de X\n","    y    = Vector de Y\n","    theta= Vector de thetas\n","    learning_rate: tasa de aprendizaje\n","    iterations = cant de iteraciones\n","\n","    Devuelve el vector final de thetas y un array con la evolución del costo a lo largo de las iteraciones\n","    '''\n","    m = len(y)\n","    cost_history = np.zeros(iterations)\n","    n_batches = int(m/batch_size)\n","\n","    for it in range(iterations):\n","        cost =0.0\n","        indices = np.random.permutation(m)\n","        X = X[indices]\n","        y = y[indices]\n","        for i in range(0,m,batch_size):\n","            X_i = X[i:i+batch_size]\n","            y_i = y[i:i+batch_size]\n","\n","            X_i = np.c_[np.ones(len(X_i)),X_i]\n","\n","            prediction = np.dot(X_i,theta)\n","\n","            theta = theta -(1/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n","            cost += cal_cost(theta,X_i,y_i)\n","        cost_history[it]  = cost\n","\n","    return theta, cost_history"],"metadata":{"trusted":true,"id":"HbodC9J645hE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["¿Cómo se está haciendo la \"evaluación\" del gradiente? ¿Cómo se está haciendo la actualización de \"los\" thetas?"],"metadata":{"id":"DdAdYyg0Z4Qe"}},{"cell_type":"code","source":["lr =0.1\n","n_iter = 200\n","\n","theta = np.random.randn(2,1)\n","\n","\n","theta,cost_history = minibatch_gradient_descent(X,y,theta,lr,n_iter)\n","\n","\n","print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n","print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))"],"metadata":{"trusted":true,"id":"PUIjhw-s45hE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,ax = plt.subplots(figsize=(10,8))\n","\n","ax.set_ylabel('{J(Theta)}',rotation=0)\n","ax.set_xlabel('{Iterations}')\n","theta = np.random.randn(2,1)\n","\n","_=ax.plot(range(n_iter),cost_history,'b.')"],"metadata":{"trusted":true,"id":"Q4gUkOWg45hE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"trusted":true,"id":"AoBetQ6X45hE"},"execution_count":null,"outputs":[]}]}