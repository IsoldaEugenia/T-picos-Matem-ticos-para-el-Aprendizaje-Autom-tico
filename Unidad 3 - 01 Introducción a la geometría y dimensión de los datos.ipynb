{"cells":[{"cell_type":"markdown","id":"eea1d25c","metadata":{"id":"eea1d25c"},"source":["# UNIDAD 3: GEOMETRÍA Y DIMENSIÓN DE LOS DATOS"]},{"cell_type":"markdown","id":"af4fda6b","metadata":{"id":"af4fda6b"},"source":["## 3.1 Introducción"]},{"cell_type":"markdown","id":"b2a4d9b0","metadata":{"id":"b2a4d9b0"},"source":["En matemática la noción de dimensión tiene diversas interpretaciones. Repasemos algunas, introduzcamos otras y veamos su relevancia en el análisis de los datos.\n","\n","- La primera definición de dimensión que vemos es la que viene del álgebra lineal: *dimensión de un espacio vectorial.*\n","\n","- Más adelante se ve una definición topológia-geométrica: la *dimensión de una variedad diferenciable* (y la definimos como la dimensión del espacio tangente en cada punto, como espacio vectorial).\n","\n","- En análisis abstracto, específicamente en la rama llamada Teoría geométrica de la medida (en Argentina hay muchos grupos de investigación que trabajan en estos temas: UBA-MDQ-STA.FE, etc), aparecen las *dimensiones fractales* cuando el objeto geométrico no admite espacio tangente.\n","\n","En cuanto a datos, podemos pensar en una noción *intrínseca* de la siguiente forma: la mínima cantidad de variables que se precisan para caracterizar localmente los datos a través de una función que los modela. Por ejemplo, si consideramos nuestro dataset como la gráfica de una función, el Teorema de la función implícita nos permitirá calcular la dimensión intrínseca. En efecto, recoremos que dicho teorema permitía, a partir de una ecuación definir a una o varias variables como función de las demás.\n","\n","Todas estas nociones de dimensión involucran la cantidad de parámetros que precisamos (local o globalmente) para caracterizar el dataset, y nos permiten así cuantificar la geometría y la complejidad del dataset. Nos da, de alguna manera, una medida de los *grados de libertad* que están en juego.\n","\n","Resulta así que estimar la dimensión es un buen primer paso en el proceso de analizar los datos y hacer predicciones sobre ellos. Dicho esto, enumeraremos los objetivos de la unidad:\n","\n","- estudiar las nociones de dimensión en datos y cómo \"copiar\" el conjunto de datos en un espacio de dimensión más baja,\n","\n","- estudiar técnicas para estimar las dimensiones,\n","\n","- analizar la forma, geometría y topología del conjunto de datos para obtener información que escapa a las nociones estadísticas,\n","\n","- entender a la geometría de los datos y el análisis de la dimensión como un buen punto de partida para que lxs matemáticxs aportemos a la ciencia de datos.\n","\n"]},{"cell_type":"markdown","id":"83423d13","metadata":{"id":"83423d13"},"source":["### 3.1.1 Sobre la geometría y dimensión de los datos\n","\n","A continuación profundizaremos sobre las nociones del párrafo anterior.\n","\n","Dependiendo del contexto (es decir, del conjunto de datos), alguna de las caracterizaciones serán más útiles que otras."]},{"cell_type":"markdown","id":"80c24b7c","metadata":{"id":"80c24b7c"},"source":["**Dimensión en el sentido lineal:**\n","\n","Pensemos en $\\mathbb{R}^3$. Sus elementos son vectores de tres componentes, y toda base tiene tres elementos distintos que generan todo el espacio mediante combinaciones lineales reales,  lo cual refleja nuestra percepción de la dimensión como una medida de grados de libertad.\n","\n","En ciencia de datos solemos considerar datos que se comportan bien en un sentido lineal, que están cerca de ser lineales.\n","\n","Por ejemplo, un conjunto de imágenes de una persona donde varía la iluminación. Las imágenes se pueden pensar como vectores de muchísimas dimensiones, y cuando hacemos combinaciones lineales de éstas podemos darle un significado: nuevas imágenes de la misma persona. Más aún, es posible probar que un objeto bajo todas las condiciones posibles de iluminación forma un cono convexo en el espacio de las imagenes, el cual se aproxima bien por un espacio vectorial de dimensión mucho más baja.\n","\n","El marco de trabajo para este tipo de problemas es el de los *grassmanianos*: el espacio vectorial que parametriza todos los subespacios vectoriales con una dimensión fija posibles de un cierto espacio vectorial."]},{"cell_type":"markdown","id":"fce87a94","metadata":{"id":"fce87a94"},"source":["**Dimensión en el sentido geométrico:**\n","\n","Hay situaciones donde considerar la dimensión lineal no es adecuado para entender y analizar los datos.\n","\n","Por ejemplo, consideremos que los datos están contenidos en el conjunto $X=\\{(x,y,z):x^2+y^2=1,z=0\\}$. ¿Cuál es su dimensión? Estamos en $\\mathbb{R}^3$, pero sabemos que los datos están sobre el plano $z=0$, entonces ¿es correcto decir que la dimensión es 2? La realidad es que nuestros datos no se *comportan bien* bajo combinaciones lineales (nos salimos del conjunto!). Entonces, estos datos no se omportan linealmente. ¿Cómo lo entendemos? Vía la teoría de variedades: si pensamos en la estructura de variedad diferenciable del conjunto $X$ observamos que la circunferencia es un objeto de dimensión 1 vía su identificación local con $\\mathbb{R}$.\n","\n","A veces es fácil detectar si los datos podemos pensarlos representados en una variedad. Las técnicas de detección se las llama *reducción no lineal de la dimensión* o *aprendizaje en variedades*. Aplicar estas herramientas resulta muchas veces en la obtención de una visualización de los datos en un espacio de dimensión baja que provee de intuición para su análisis, entre otras ventajas. Veremos algunas técnicas y aplicaciones."]},{"cell_type":"markdown","id":"428805cd","metadata":{"id":"428805cd"},"source":["**La maldición de la dimensionalidad**\n","\n","La maldición de la dimensionalidad es un concepto que introdujo Bellman en 1957 en el contexto de programación dinámica (ver https://en.wikipedia.org/wiki/Curse_of_dimensionality). En aprendizaje automático podemos interpretarlo de la siguiente forma: hay que tomar precauciones cuando se trabaja en dimensiones muy altas puesto que en ellas los volúmenes se incrementan de manera exponencial dejando a los datos escasos, insuficientes para realizar un análisis adecuado.\n","\n","En general, para que un modelo aprenda, la cantidad de datos requeridos debería también crecer exponencialmente con la dimensión. Es por eso que obtener una representación significativa en dimensión baja de los datos es, generalmente, un paso fundamental para poder aprender de los datos.\n","\n"]},{"cell_type":"markdown","id":"c1c11a5f","metadata":{"id":"c1c11a5f"},"source":["**Dimensión intrínseca**\n","\n","El aprendizaje en variedades busca encontrar una representación en dimensiones bajas para un dataset sobre el precepto de que si bien los datos seguramente residen en un espacio ambiente de grandes dimensiones, a menudo yacen o están muy cerca de una variedad de baja dimensión.\n","\n","La dimensión intrínseca se corresponde precisamente con esta dimensión baja, con los *grados de libertad*, y el aprendizaje en variedades tiene por objetivo descubrir esa dimensión, y para eso hay varias técnicas cuya tarea consiste en asegurar que este espacio de baja dimensión preserve alguna cantidad de interés:\n","- MDS (multidimensional scaling) busca preservar distancias\n","- Isomap busca preservar distancias geodésicas\n","- UMAP (uniform manifold approximation and projection) busca preservar la geometría local del dataset\n","- t-SNE (t-distributed stochastic neighbou embedding) caracterizan similaridades de puntos vía distribuciones de probabilidad y busca representaciones que las preserven\n","- LLE (locally linear embeddings) y Laplacian eigenmaps buscan preservar la estructura local\n","- PCA (principal component analysis) busca preservar la varianza.\n","\n","Estudiaremos algunas de ellas.\n","\n","En general aprendemos sobre la dimensión y estructura de nuestros datos mapeando a un espacio de dimensión más baja, sobre todo cuando tales aplicaciones pueden ser efectuadas con una pérdida relativamente pequeña en términos de lo que deseamos preservar. Cuando obtenemos una representación de los datos en una variedad, consideramos la dimensión de la variedad como la dimensión de los datos, que localmente coincida con los grados de libertad que caracterizan esa dimensión."]},{"cell_type":"markdown","id":"f173aaa7","metadata":{"id":"f173aaa7"},"source":["**Dimensiones fractales**\n","\n","Para ciertos conjuntos de datos es más apropiada la noción de dimensión fractal, por ejemplo cuando los datos no yacen en una subvariedad. El concepto de de dimensión fractal es de Mandelbrot en 1967 cuando estudia objetos auto-similares: el objeto es similar a una parte de si mismo, por ejemplo los copos de nieve, el brócoli romanesco o las líneas costeras. La matemática aquí es muy precisa (y hermosa!), pero por ahora con una visión wiki nos sirve para seguir adelante https://en.wikipedia.org/wiki/Fractal_dimension.\n","\n","Hay muchas definiciones de dimensiones fractales diferentes que suelen coincidir en conjuntos \"buenos\".\n","- La dimensión de Hausdorff: Hausdorff propuso esta noción de dimensión pensando en el conjunto de Cantor, y le da $0<dim_H(C)=log_3(2)<1$. Para calcularla, se construye primero una familia de medidas exteriores (cubrimientos) parametrizadas dos valores, tomando ínfimo en respecto de uno de esos valores se construye una medida y luego tomando ínfimo sobre la otra se calcula la dimensión. O sea, es bastante engorroso.\n","- La dimensión box-counting o de Minkowsy: se obtiene relajando un poco la definición de dimensión de Hausdorff, es algo así como el ínfimo del número de cubos de longitud fija que se precisan para cubrir el conjunto cuando la longitud se acerca a cero. En lugar de cubos puede, alternativamente, considerarse bolas cerradas de radio fijo.\n","- La dimensión packing: es una noción dual a la de Hausdorff, en lugar de cubrir por conjuntos se \"meten\" conjuntos dentro, se considera cuántos cubre (no cúantos se necesitan para cubrirlo).\n","- La correlation dimension: mide la dimensionalidad del espacio que ocupa un conjunto aleatorio de puntos.\n","- etc.\n"]},{"cell_type":"markdown","id":"NY1hnt-wQ-zB","metadata":{"id":"NY1hnt-wQ-zB"},"source":["\n","\n","---\n","\n","\n","Estas notebooks corresponden a la asignatura electiva para la Licenciatura en Matemática del Departamento de Matemática de la Escuela de Cs. Exactas y Naturales, de la Facultad de Cs. Exactas, Ingenieria y Agrimensura (FCEIA) de la Universidad Nacional de Rosario, Argentina. Año 2023.\n","\n","Fueron confeccionadas a tal fin por las docentes investigadoras de la FCEIA Isolda Cardoso y Jorgelina Walpen. Este trabajo de análisis, estudio, recopilación, traducción, armado, pruebas y errores, nos ha llevado mucho tiempo. Cuando hay tanta información disponible es complicado extraer lo que, al menos para nosotras, es relevante. Hay muchísimos otros recursos y tutoriales, nosotras armamos este.\n","\n","Si estas notebooks son reproducidas, solicitamos citar la fuente.\n","\n","Dejamos nuestras páginas laborales y repositorios de GitHub. Seguramente en un futuro las pondremos a disposición en el Repositorio Hipermedial de la UNR.\n","\n","Isolda: Te dejo mi página laboral https://www.fceia.unr.edu.ar/~isolda/ y mi GitHub https://github.com/IsoldaEugenia. Sentite libre de contactarme.\n","\n","Jorgelina: https://www.fceia.unr.edu.ar/~walpen/ y  https://github.com/JorWalpen"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":5}