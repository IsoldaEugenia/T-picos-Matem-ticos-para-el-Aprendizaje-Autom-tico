{"cells":[{"cell_type":"markdown","metadata":{"id":"qxWvhYY1wEIp"},"source":["# Unidad 2 - 02 Descenso del gradiente en procesos de ML"]},{"cell_type":"markdown","metadata":{"id":"6Umg9RPv0WvE"},"source":["## 1. Introducción\n","\n","En términos generales, la optimización es una herramienta usada en todo proceso de investigación operativa, ya sea para ajustar un modelo o tomar decisiones basadas en un modelo. Pensemos en los siguientes ejemplos básicos:\n","* una compañía de tarjetas de crédito que quiere predecir compras fraudulentas,\n","* un hotel que quiere definir precios de sus habitaciones basándose en la demanda de períodos anteriores,\n","* un investigador de la salud que quiere identificar poblaciones con perfiles de riesgo similares, etc.\n","\n","La ciencia de datos (y nosotros, los científicos) abordamos estos problemas transformándolos en problemas de optimización. En el caso de problemas de aprendizaje supervisado como los modelos de regresión lineal, la optimización estuvo presente para resolver el problema de encontrar los parámetros que \"mejor ajustan\" el modelo a los datos. Cuando se trata de aprendizaje no supervisado, como los problemas de clustering, la optimización aparece, por ejemplo, para maximizar similitudes entre elementos de un mismo \"cluster\".\n","\n","### Optimización Convexa\n","\n","Una clase muy particular de problemas de optimización pertenecen al grupo de Optimización convexa. En este universo se \"cocinan\" gran parte de los algoritmos de machine learning, incluyendo regresión lineal, regresión Lasso, SVM (support vector machines), etc.\n","\n","Un problema de optimización convexo, tiene el siguiente formato:\n","\n","$$\\begin{array}{ccc}\\underset{x}{min} &f(x)\\\\\n","s.a.& g_i(x)\\leq 0 &\\forall i=1,2,..,m.\\end{array} (1) $$\n","\n","donde:\n","* $f$ es una función convexa\n","* La región factible (definida por las restricciones) es un conjunto convexo.\n","\n","Obs: claramente aplica el caso donde $min$ se reemplaza por $max$. El caso particular donde las $g_i$ son convexas también, se conoce como **forma estándar**\n","\n"]},{"cell_type":"markdown","source":["### Breve repaso\n","\n","* Un conjunto $X\\subset \\mathbb R^n$ es convexo si para todos $x_1,x_2\\in X$, y para todo $\\alpha\\in(0,1)$ resulta: $\\alpha x_1+(1-\\alpha)x_2 \\in X$.\n","\n","* Propiedades de los conjuntos convexos: (1)la intersección de convexos es convexa; (2) Si $X\\subset\\mathbb R^n$ es un conjunto convexo, entonces la transformación afín $\\{Ax+b\\in \\mathbb R^n | x\\in X\\}$ es un conjunto convexo.\n","\n","* Dado un conjunto convexo $X$, una función $f:X\\rightarrow \\mathbb R$ es una función convexa en $X$ si $\\forall x_1,x_2\\in X$ y $\\alpha\\in (0,1)$, resulta: $$f(\\alpha x_1+ (1-\\alpha)x_2)\\leq \\alpha f(x_1)+(1-\\alpha) f(x_2)$$\n","\n","* Si $f_1,f_2,...,f_k$ son funciones convexas y $w_1,w_2,...,w_k \\geq 0$ son pesos no negativos, entonces $w_1f_1(x)+w_2f_2(x)+...+w_kf_k(x)$ es convexa\n","\n","* Si $\\{f_a\\}_{a\\in\\mathcal A}$ es una familia de funciones convexas, entonces $f(x)=sup_{a\\in\\mathcal A}f_a(x)$ es una función convexa\n","\n","* Si $f(x)$ es una función convexa, entonces $f(Ax+b)$ es una función convexa.\n","\n","* Si $f(x)$ es 2 veces continuamente diferenciable, entonces $f$ es convexa si y sólo si su matriz Hessiana es semidefinida positiva $\\forall x\\in X$.\n","\n","Como es el caso de los problemas lineales, los problemas convexos exhiben una fuerte estructura que nos permite resolverlos de manera eficiente. Por un lado la convexidad de la región factible y la función objetivo implican que un óptimo local es un óptimo global del problema. Además, dadas una solución factible $x$ y una solución óptima $x^*$, por la convexidad de la región factible, \"la línea\" entre $x$ y $x^*$ está contenida en la región factible. Luego, la convexidad de la función objetivo, \"moverse\" a lo largo de dicha \"línea\" es una dirección de mejora.\n","\n"],"metadata":{"id":"57BnBpdb6Efa"}},{"cell_type":"markdown","source":["## Métodos de optimización de primer orden\n","\n","Si ignoramos las restricciones del problema convexo planteado en (1), y asumimos que $f$ es diferenciable, el problema se convierte en el problema de *minimizar una función convexa, diferenciable* que puede resolverse facilmente usando **descenso del gradiente**.\n","\n","### Descenso del gradiente\n","\n","En cada paso $t$ del método del *descenso del gradiente* estamos en algún punto $x_t$ y encontramos $x_{t+1}$ moviéndonos en la dirección opuesta del gradiente en dicho punto $x_t$, es decir, $x_{t+1}=x_t-\\eta_t\\nabla f(x_t)$. Definiendo la longitud del paso $\\eta_t$ de manera apropiada, la sucesión converge a una solución óptima.\n","\n","* La convergencia puede mejorarse usando el **método de Newton** para aproximar $f$ en cada punto $x_t$.\n","\n","Sin embargo, rápidamente llegan las dificultades.\n","\n","1) Debemos mantener la factibilidad en cada paso del algoritmo\n","\n","2) Aún en los escenarios más comunes de la ciencia de datos, $f$ no es diferenciable.\n","\n","###No diferenciabilidad de $f$\n","\n","En este caso, la alternativa al descenso del gradiente, para obtener una aproximación lineal de $f$, es el método de **subgradientes**, para obtener una dirección de búsqueda.\n","\n","Un subgradiente de $f$ en $x_t$ es un vector $v_t$ tal que:\n","$$f(x)-f(x_t)\\geq v_t(x-x_t) \\;\\forall x$$\n","\n","En otras palabras, $v_t$ nos da una cota inferior para $f$ que se asume por igualdad en $x=x_t$.\n","\n","Por ejemplo, $v\\in[-1,1]$ es un subgradiente para $f(x)=|x|$ en $x=0$.\n","\n","Ver imagen.\n","\n","\n","En el método de subgradientes, el algortimo usa el subgradiente como dirección de búsqueda. En cada paso en el punto $x_t$, el algoritmo define $x_{t+1}=x_t-\\eta_t v_t$ con $\\eta_t$ una longitud del paso. Si $f$ es diferenciable, $v_t$ es el gradiente en $x_t$ y el algortimo equivale al de **máximo descenso**. Si bien este último es más lento que el método de Newton, converge a una solución óptima.\n","\n","En caso general cuando $f$ no es diferenciable, como $v_t$ es un subgradiente, no está garantizado que $f$ \"mejore\" en cada paso $x_t$. Sin embargo, si el paso $\\eta_t$ se elige de manera tal que $\\eta_t\\rightarrow 0$ y $\\sum_t \\eta_t=\\infty$, tenemos resultados de convergencia a una solución óptima.\n","\n","El método de subgradientes mencionado, no tiene en cuenta las restricciones del problema, por lo cual no se ocupa de la factibilidad de cada punto $x_t$. Para abordar este caso, tenemos el **método de subradientes proyectado**, el cual garantiza la factibilidad del punto $x_{t+1}$ proyectándolo sobre la región factible. La proyección puede computarse de manera eficiente para diferentes tipos de conjuntos convexos y mientras la sucesión $\\eta_t$ cumpla con las condiciones mencionadas ($\\small\\eta_t\\rightarrow 0$ y $\\small\\sum_t \\eta_t=\\infty$), habrá convergencia a una solución óptima.\n","\n","La simpleza del método de subgradientes hace que sea tentador su uso, aún siendo menos eficiente que los métodos de punto interior. Más áun, estos métodos pueden correrse sin dificultad para problemas de gran tamaño dado que no consumen demasiada memoria.\n","\n","Una alternativa para mejorar el tiempo de procesamiento en el caso de problemas con gran cantidad de datos es el **método del gradiente estocástico**. Muchas funciones en el contexto del machine learning pueden escribirse como suma de funciones: $f(x)=\\frac{1}{n}\\sum_i f_i(x)$, donde $f_i$ es una **función de pérdida** de la i-ésima observación. La misma mide qué tan bien el modelo se ajusta a dicha observación. Se propone evaluar el gradiente (o subgradiente) de una muestra aleatoria de $f_i's$, únicamente. Si se asumen ciertas propiedades sobre las $f_i's$, la muestra debería dar una buena estimación del gradiente (o subgradiente) en el caso de data sets de gran tamaño.\n","\n","El gradiente de la suma es la suma de los gradientes... ¿con subgradientes pasa lo mismo?\n","\n","\n","### Dualidad y condiciones de optimalidad\n","\n","Como en el caso de problemas lineales, podemos usar dualidad para dar más estructura al conjunto de soluciones óptimas y desarrollar algoritmos más eficientes. Asociar un multiplicador de Lagrange $y_i$ a cada restricción de desigualdad permite escribir el siguiente problema dual:\n","$$\\begin{array}{ccc}\\underset{y}{max} &\\mathcal L(y):=\\underset{x\\in\\mathbb R^n}{inf}\\left[f(x)+\\sum\\limits_{i=1}^my_ig_i(x)\\right]\\\\s.a.& y\\geq 0\n","\\end{array}(2)$$\n","\n","La función objetivo en $(2)$  puede interpretarse como aquella que modeliza la búsqueda de la cota inferior más ajustada para $(1)$.\n","\n","Se puede mostrar que los problemas convexos satisfacen el principio de **dualidad débil**. Esto quiere decir que: *para todo $x$ factible de $(1)$ e $y$ factible de $(2)$, $f(x)\\geq \\mathcal L(y)$*. Sin embargo a diferencia del caso lineal, los problemas convexos satisfacen la **condición de dualidad fuerte** cuando se verifican **condiciones de regularidad**\n","\n","Existen varias formas de mostrar \"regularidad\". Un test mas o menos simple es chequear la **condición de Slater**, es decir, mostrar que existe un punto \"estrictamente factible\": $x/g_i(x)<0\\;\\forall i=1,..,m.$\n","\n","Obs: Podemos leer algo más sobre restricciones \"calificadas (o qualification constraints) en el Capítulo 50 de Optimización no lineal de las notas de Jean Gallier and Jocelyn Quaintance.\n","\n","Si el problema convexo satisface alguna condición de regularidad y tanto el problema primal como el dual tienen un conjunto factible no vacío, entonces existen $x^*$ e $y^*$ tales que $f(x^*)=\\mathcal L(y^*)$. Afortunadamente, la mayoría de los problemas convexos razonables, cumplen con alguna condición de regularidad.\n","\n","Para el problema primal $(1)$ y su dual $(2)$ se pueden escribir las condiciones de optimalidad de Karush-Kuhn-Tucker (KKT). Las mismas son necesarias y suficientes para la optimalidad, en el contexto convexo. Por ejemplo en el caso de que el problema $(1)$ esté en forma estándar y satisfaga la condición de Slater mencionada.\n","\n","**Condiciones KKT para el caso convexo**\n","\n","Considere un problema convexo como en $(1)$ en forma estándar, que verifique Slater y tal que $f$ y $g_i$ ($\\forall i=1,..,m$), sean continuamente diferenciables en algún punto $x\\in \\mathbb R^n$. Luego, $x$ es una solución óptima si y sólo si existe $y\\in\\mathbb R^m$ tal que:\n","\n","1. $\\nabla \\mathcal L(x)=0$ (estacionariedad)\n","2. $x$ es factible para $(1)$ (factibilidad primal)\n","3. $y\\geq 0$ (factibilidad dual)\n","4. $y_ig_i(x)=0 \\;\\forall i=1,2,..,m$ (complementariedad)\n","\n","\n","Los algoritmos de optimización convexa desarrollados entorno a este enfoque son conocidos como métodos de punto interior. Iteración a iteración se matiene la factibilidad primal y la factibilidad dual expresadas en las condiciones 2 y 3 y se trabaja de manera tal de poder garantizar la complementariedad 4 de las condiciones KKT.\n","\n","\n"],"metadata":{"id":"iyAsI-D5HMCj"}},{"cell_type":"markdown","source":["### Descenso del gradiente\n","\n","El desceso del gradiente es una herramienta que puede utilizarse en más de una de las etapas del flujo de trabajo de un proceso de machine learning. La misma juega un rol fundamental en los métodos de redes neuronales, al mismo tiempo que permite extensiones de las técnicas de regresión lineal, entre otras cosas.\n","\n","####Funciones de pérdida\n","\n","Como a esta altura ya debe estar claro, el principal objetivo del machine learning es aprender de los datos, razón por la cual debemos contar con alguna \"medida\" para cuantificar el desempeño de dicho aprendizaje: las funciones de pérdida $L(\\hat y,y)$. La misma representa \"la pérdida\" de predecir $\\hat y$ en lugar de $y$.\n","\n","En los problemas de regresión trabajamos con la tradicional función de pérdida cuadrática: $L(\\hat y,y)=(\\hat y-y)^2$. Por otro lado, en los problemas de clasificación, usamos la función de pérdida para penalizar predicciones incorrectas. Un ejemplo estándar es la \"función de pérdida $0-1$\", $L(\\hat y,y)=1_{\\hat y\\neq y}$:\n","$$1_{\\hat y\\neq y}=\\left\\{\\begin{array}{cl}1& si \\;\\hat y\\neq y\\\\ 0 & si\\; \\hat y= y\\; \\end{array}\\right.$$\n","\n","En algunos contextos es más apropiado el uso de funciones de pérdida asimétricas: declarar una operación bancaria fraudulenta como correcta es más dañino que declarar una operación correcta como fraudulenta.\n","\n","Si bien el deseo es minimizar la función de costo (o pérdida) durante el proceso de aprendizaje, concentrarse solo en esa tarea puede llevar al bien conocido **\"overfitting\"**. Lo que en realidad queremos es un modelo que *generalice bien cuando se trata de datos no vistos previamente*.\n","\n","En muchos casos, la función de costo es modificada para atender \"los conceptos de los datos\" o bien para (siempre que sea posible) \"convexificarla\".\n","\n","**Una herramienta de optimización poderosa**\n","\n","Una vez que tenemos la función de costo asociada al problema de machine learning, nuestro objetivo siguiente es encontrar los parámetros del modelo que minimizan dicho costo. En algunos contextos (no muy frecuentes), ésto puede hacerse de manera analítica, (veremos un ejemplo en la sección siguiente)????? pero en la mayoría de los casos la optimización se lleva adelante recurriendo a técnicas numéricas, como el **descenso del gradiente**.\n","\n","Sea $J(\\theta)$ una función convexa, diferenciable, que representa el costo en función del parámetro $\\theta$ en nuestro modelo de aprendizaje automático. ¿Quién es $\\theta$ que minimiza $J(\\theta)$? Comenzamos con una estimación inicial $\\theta_0$ para $\\theta$ y de manera iterativa actualizamos la estimación usando el siguiente esquema:\n","$$\\theta_{i+1}=\\theta_i-\\alpha\\nabla J(\\theta_i) \\;\\;\\;(3)$$\n","\n","hasta su convergencia (criterio de parada: $|\\theta_k-\\theta_{k+1}|<\\varepsilon\n",", \\varepsilon$ suficientemente pequeño). $\\alpha$ representa la longitud del paso o **tasa de aprendizaje**. Intuitivamente, $\\theta$ se actualiza en la dirección del máximo descenso. La elección de la tasa de aprendizaje, no es tarea despreciable, de ella depende la convergencia al mínimo (o máximo). Pasos muy cortos pueden volver muy lento el proceso iterativo, y pasos muy largos pueden hacer que el algortimo diverja. Una elección que se usa y suele funcionar (para J's con ciertas bondades) es $\\alpha=\\dfrac{1}{L}$, con $L$ la constante de Lipschitz del gradiente, la cual satisface:\n","$$||\\nabla J(\\theta)-\\nabla J(\\theta')||\\leq L||\\theta-\\theta'||\\;\\;\\forall \\theta, \\theta'.$$\n","\n","Observaciones:\n","* Cada iteración del esquema $(3)$ implica calcular el gradiente $\\nabla J(\\theta)$. Dicha función $J$ inevitablemente involucra al subconjunto de datos separados para el entrenamiento, el cual puede ser muy grande. Esta forma de usar el método del gradiente se conoce como **batch gradient descent** y tiene la desventaja de consumir mucho tiempo de cálculo para training sets grandes. De manera alternativa se puede actualizar $\\theta$ utilizando un único item del conjunto de entrenamiento en cada paso y elegir a dicho item, de forma aleatoria. Este enfoque se conoce como el **descenso del gradiente estocástico**. Funciona bien en la práctica, luego de que el algoritmo haya pasado varias veces por los datos.\n","\n","El método del descenso del gradiente puede usarse en algunos escenarios no convexos, pero no se puede asegurar convergencia a un óptimo global; muchas veces el esquema puede quedarse \"atorado\" en un mínimo local.\n","\n","Existen muchas variantes para el descenso del gradiente, incluso para el caso no-convexo, muchas de ellas se usan, por ejemplo, en modelos de redes neuronales.\n"],"metadata":{"id":"TMKwFvADhwdv"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}